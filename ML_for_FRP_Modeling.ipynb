{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purpal9/RF_FRP_v1/blob/main/ML_for_FRP_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d70R-p3Eie0"
      },
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqcGiC95DJ3T"
      },
      "source": [
        "# Machine Learning for Fire Radiative Power Modeling\n",
        "**Christina Kumler\n",
        "Fall 2023 - Spring 2024**\n",
        "\n",
        "Welcome to the machine learning (ML) for fire radiative power (FRP) notebook! In this two-part notebook, we will explore how to train a ML model called a Random Forest (RF) to model FRP one hour into the future. This notebook has several sections and two main parts. In the Envionrment Part 0, we will import all the necessary enviornments to run through the notebook.\n",
        "\n",
        "In Part 1, we will go through a small example to learn how to obtain the input files used to create an machine learning ready (ML-ready) dataset, and how to process the data into a dataset. This is the section that shows the process for developing the dataset, but to keep the notebook simple, this is not the dataset that will be used in part 2 when we learn how to train and test a RF model.\n",
        "\n",
        "In Part 2, we begin by importing a pre-processed, ML-ready dataset. Then, we will go through the steps on how to train and test a RF model to predict hourly FRP. After, we will demonstrate some package options for analyzing your RF model's performance.\n",
        "\n",
        "If you are interested in using a pre-processed dataset to train RF models, please run through section 0 and then skip forward to section 2.\n",
        "\n",
        "By the end of this tutorial, you will have learned how to pull RAP data using Herbie, import RAVE data, and process them into an ML-ready dataset. Additionally, you will have learned how to use that ML-ready dataset in practice to train a RF model and visualize the performance of that model.\n",
        "\n",
        "A brief tutorial of the colab notebook space. To the right, we have information on the colab computing space. In this notebook, we will change between standard compute resources and GPUS. This will be done by clicking the **RAM and Disk Icons** later. You can also click the up arrow and it will hide the header of the notebook. To the left, there are icons that include:\n",
        "> **list** or Table of Contents, which is where you can jump between sections of the colab notebook. Note that Sections 1 and 2 can run independelty, but often the subsections will require running coding blocks in previous sections.\n",
        "> **magnifying glass** or Find and Replace\n",
        "> **{x}** or Variables, which is where the local python variables that are created in the coding blocks will be visible. This is useful for checking the types and sizes of the variables in the coding process.\n",
        "> **key** or Secrets\n",
        "> **folder** or Files, where the files that are pulled as well as created will be saved in the colab notebook space.\n",
        "\n",
        "\n",
        "Enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EIFV3CkNfz5"
      },
      "source": [
        "# 0.1 Setting up a python environment\n",
        "\n",
        "**In this section, we will import some necessary tools to set up an environment that can download relevent input data, perform data analysis and processing steps, and train RF ML models. Because we are working in a colab notebook, the environment and resources stay connected to this environment and will not impact your local machine. To the left of the notebook, you will find the toolbar that shows the layout of the notebook, a list of current variables, and any additional files contained in the notebook. Note, you should always practice caution when installing packages or tools onto a local machine, as version mismatches can sometimes impact previously developed code. Always practice good code and environment management.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sirE4SusEOTs"
      },
      "source": [
        "The following will install wget, a useful tool for \"getting\" data stored on an internet server by pointing to its location and downloading it to this notebook. In this colab notebook, all coding will be done in python. One thing to note is if you see a \"!\" symbol before a block of code, this is known as the magic function and will run terminal-like codes from inside python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMw4NfdRdB30"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "import wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEYwSgvqdG-t"
      },
      "source": [
        "Next, we check that things installed correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACeGewUtdLXP"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKe25h91dWet"
      },
      "source": [
        "Once we have confirmed that things installed correctly, we will install a tool developed for pulling a variety of weather model outputs called: HERBIE. The source code, documentation, and tutorials can be found here: [HERBIE](https://herbie.readthedocs.io/en/stable/user_guide/index.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC45yAMVDEl4"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/blaylockbk/Carpenter_Workshop.git\n",
        "!pip install ecmwflibs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mpPbZvtgav8"
      },
      "outputs": [],
      "source": [
        "!pip install herbie-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrdu1s31QZJk"
      },
      "source": [
        "We will install one last python tool in this section for reading data that is collected in a specific format called NetCDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w1n3RK7io33"
      },
      "outputs": [],
      "source": [
        "!pip install netCDF4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xarray[complete]"
      ],
      "metadata": {
        "id": "3PSwsZPRKNlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cfgrib"
      ],
      "metadata": {
        "id": "TqJwFLEHeJnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j4O5lRiNwak"
      },
      "source": [
        "# 1.1 Importing desired data\n",
        "\n",
        "**In this section, we set up the code that gets the desired variable inputs. It is important to note that should you desire to train your own machine learning model, and later use this model in your own applications, which is refered to as _inference_ , then you will need to pull as much data as possible and keep the data inputs consistent.**\n",
        "\n",
        "**We will be extracting two types of data here. The first is the input data from the RAP weather model. These files are very large, so this code is designed to extract specific variables from the full RAP files. If you desire additional variables, please reference RAP's material to identify the variable names. The second data were are inputting is label for our ML model, which is the RAVE fire radiative power (FRP) data.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trSZh48Yiq3F"
      },
      "source": [
        "1.1.1 Here we pull the RAVE data [Fangjun Li et al.] as our satellite FRP data source. While  RAVE is fully operational, there is no online portal for making data requests or pulls. In this tutorial, we will collect a sample daily RAVE data file, from Zenodo, by using a wget command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3rd4RjDCjKw"
      },
      "outputs": [],
      "source": [
        "RAVE_nc = wget.download('https://zenodo.org/records/10694507/files/RAVE-HrlyEmiss-3km_v2r0_blend_s202209010000000_e202209012359590_c202308071356470.nc?download=1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9gFiJd4ffwY"
      },
      "source": [
        "In this notebook, we will be making a an example dataset that includes 2 hours of data. Since RAVE is stored in a daily netcdf file, we only need to read one file into the colab space. We will save this file into python using an extension called netCDF4. To learn more about the netcdf file strcuture, please visit: https://www.unidata.ucar.edu/software/netcdf/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehsXUonhR8vB"
      },
      "outputs": [],
      "source": [
        "import netCDF4 as nc\n",
        "\n",
        "rave_data = nc.Dataset(RAVE_nc)\n",
        "print ('opened RAVE file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgnbHeRd3LcM"
      },
      "source": [
        "Congratulations! We have successfully pulled a daily RAVE data file and stored it into our notebook space as \"rave_data\" which can be referenced later as we process our own miniature RF-ready dataset. If you'd like to see where the RAVE file went, you can look at the left of the colab notebook and click the **folder icon** and see that the file now is saved into the colab notebook space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFSWt8LACrMR"
      },
      "source": [
        "1.1.2 Next, we use HERBIE in the processing script to access the RAP data. Note that documentation specific to HERBIE can be found in the previous section. RAP data is fairly messy, with having different naming conventions and products, etc. HERBIE is a very useful tool at helping navigate some of these nuances. When building your own training dataset, be sure to know what input data you are using, if they change with model updates, if their naming changes, if they are not avaible on the time scale you need them, etc.\n",
        "\n",
        "There are two ways to use HERBIE. First, we could use HERBIE to save the raw RAP file into a working or sub directory. However, this tutorial will use HERBIE to access the RAP data in a HERBIA object, which means we will never actually have to save and reopen the RAP data.\n",
        "\n",
        "Below is an example of how this will look later in the notebook. We are going to grab a single file for our date of interest: September 1, 2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmxTcEI3infm"
      },
      "outputs": [],
      "source": [
        "from herbie import Herbie\n",
        "from toolbox import EasyMap, pc\n",
        "from paint.standard2 import cm_tmp\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import xarray as xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP2bxem6yH5Q"
      },
      "outputs": [],
      "source": [
        "#H = Herbie(\"2022-09-01\", model=\"rap\")\n",
        "H = Herbie(\"2024-01-01 00:00\", model=\"rap\", fxx=0)\n",
        "\n",
        "## Uncomment the below two lines of code by deleting the \"#\" to test saving the\n",
        "## RAP file localling into the notebook:\n",
        "\n",
        "RAP_file = H.download()\n",
        "print('Here is the location in the notebook for RAP file: ', RAP_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cfgrib\n",
        "import xarray as xr\n",
        "\n",
        "# Works but have to index\n",
        "#ds = cfgrib.open_datasets(RAP_file)\n",
        "\n",
        "# THIS WAY IS MY WAY THAT DOESN'T WORK IN COLAB DUE TO PYNIO ERRORS\n",
        "#grbs = xr.open_dataset(RAP_file,\n",
        "#         engine='pynio',\n",
        "#         lock=False,\n",
        "#         backend_kwargs=dict(format=\"grib2\"),\n",
        "#         )\n",
        "\n",
        "# WORKS but potentially limited variables?\n",
        "grbs = xr.open_dataset(RAP_file,\n",
        "         engine=\"cfgrib\",\n",
        "         filter_by_keys={'typeOfLevel': 'atmosphere'}\n",
        "         )\n",
        "grbs4 = xr.open_dataset(RAP_file,\n",
        "         engine=\"cfgrib\",\n",
        "         filter_by_keys={'typeOfLevel': 'maxWind'}\n",
        "         )\n",
        "\n",
        "#print ('opened RAP file ', RAP_file)\n",
        "#H.inventory()"
      ],
      "metadata": {
        "id": "Q5ELCTD3Cikf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if I get pynio to work, this is how to open file\n",
        "temp2m = ds['TMP_P0_L103_GRLL0'].values.flatten()\n",
        "#temp2m = ds[1].values.flatten()"
      ],
      "metadata": {
        "id": "3JkV0s1uDgNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuMfKS2VvZiE"
      },
      "source": [
        "Now we take that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR9emMVbifdd"
      },
      "source": [
        "The following will list out the aviabile variables/products for us to use from the RAP file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUFTvJ5Qix_c"
      },
      "outputs": [],
      "source": [
        "H.PRODUCTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSG-VvjLSvl5"
      },
      "source": [
        "Below is a demonstration on how we will use HERBIE in the processing code to extract specific variables from the RAP data. In this example, we are extracting data from RAP's 2m temperature, as well as the latitude and longitude values, on September 1 2022 at 0UTC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqfA4YExQIoO"
      },
      "outputs": [],
      "source": [
        "# Make a dataset from desired variable\n",
        "ds = H.xarray(\":(?:TMP|RH):2 m\", remove_grib=False)\n",
        "#ds = H.xarray(\":(?:TMP|DPT|RH):\", remove_grib=False)\n",
        "# DOES NOT WORK ds = H.xarray(\":(?:TMP\\|DPT\\|RH):\", remove_grib=False)\n",
        "ds\n",
        "\n",
        "# these are all numpy vectors\n",
        "lat = ds.latitude.values\n",
        "lon = ds.longitude.values\n",
        "tmp_2m = ds.t2m.values\n",
        "RH_2m = ds.r2.values\n",
        "\n",
        "print('Print a single temp from index [260,34]: ', tmp_2m[260, 34])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCm9Mrr1CoZa"
      },
      "source": [
        "Congratulations, we have successfully pulled input variables from a RAP file using HERBIE. We will use similar commands and tools in section 1.2 when we process the dataset. It should be noted that HERBIE is a tool, and if you prefer to substitute this method with pynio in your own practice outside of this colab notebook, that would work the same way. Below is code how to open a RAP grib2 file called *RAP_FILE* and store it into a dataset grbs:\n",
        "\n",
        "\n",
        "```\n",
        "grbs = xr.open_dataset(RAP_FILE,\n",
        "         engine='pynio',\n",
        "         lock=False,\n",
        "         backend_kwargs=dict(format=\"grib2\"),\n",
        "         )\n",
        "```\n",
        "\n",
        "If you have exatracted the whole RAP file, or even a subset, then you will be able to reference variables by the names provided in RAP documentation: https://www.nco.ncep.noaa.gov/pmb/docs/grib2/grib2_doc/grib2_table4-2.shtml. When you know which variables are of interest, then this is code on how to select specific varialbes and their values to store into a xarray.\n",
        "\n",
        "```\n",
        "temp2m_var = grbs['TMP_P0_L103_GRLL0'].values\n",
        "```\n",
        "\n",
        "would store temperature at 2m into a variable named *temp2m_var*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVX__HnUntiS"
      },
      "source": [
        "# 1.2 Processing the Data\n",
        "\n",
        "**In this section, we will process the RAVE data that we imported with RAP variables to create an ML ready dataset. There are different ways we could chose to build an ML-ready dataset based on what type of ML model we would like to train. For modeling hourly FRP, we are interested in each grid cell where there is a non-zero FRP point from the time step before. We could either design an ML-ready dataset on the whole domain or we can store data in a tabular method where each hour there are enteries where there are non-zero FRP points. To train the RF ML model, we can chose to store the ML-ready dataset in a tabular method. This means that we will process the inputs for each non-zero FRP point in the RAVE dataset, by each hour, and pair it with the correspoinding RAP gridcell weather variable data and the latitude/longitude location and hour of day. As we do this, we will then store the combined data into a tabular csv format. For this notebook, we made this choice because we seek to model hourly FRP once a fire is burning. We are not interested in zero values for FRP and therefore not interested in RAP gridcells that have zero values for FRP.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jIlzhjG57aj"
      },
      "source": [
        "1.2.0 First, we will load the python tools needed to perform the data merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSXYJyoM57G2"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import netCDF4 as nc\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import netCDF4 as nc\n",
        "from pyproj import Proj\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from scipy.spatial.distance import cdist\n",
        "import gc\n",
        "import numpy.ma as ma\n",
        "import pygrib\n",
        "import glob\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIz_wignPFeE"
      },
      "source": [
        "Then we create a colab folder to store, or output, the processed hourly csv files in once they have been created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozMigCCJPMk5"
      },
      "outputs": [],
      "source": [
        "!mkdir RAVE_RAP_proc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTraYmjA6Irt"
      },
      "source": [
        "1.2.1 In the following subsection, we will write program blocks that will be used in a main coding block in section 1.2.2 that will merge RAP and RAVE data by hour and location to make hourly files that contain an FRP, location, time of day, and weather variables. But first, we start with defining the location of the output csv files and the columns, which is the future list of inputs, of the dataframes that we will store into the csv files. We have restated the file names, mostly for the benefit of this notebook and keeping track of which files we are using. If more files are wished to be used in an interative process, this block of code could be modified with directory locations instead of specific file names or variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwBsh7PUw2cd"
      },
      "outputs": [],
      "source": [
        "# Where to store the files?\n",
        "out_directory = './RAVE_RAP_proc'\n",
        "# What was the input RAVE file?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYmX82wd7U5L"
      },
      "outputs": [],
      "source": [
        "# define columns to be output with files\n",
        "columns = ['dateTime', 'id', 'lat', 'lon', 'FRP',\n",
        "        'temp2m', 'DPT', 'SPFH',\n",
        "        'uwind', 'vwind', 'gust',\n",
        "        'SOILW', 'VGTYP']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTV_4gLw89oi"
      },
      "source": [
        "Next, we define some *functions* that are used by the main code to process the files together. Functions are useful in python because they can be used at any part of the code and called by different scripts and codes.\n",
        "\n",
        "The first function is called:\n",
        "\n",
        "```\n",
        "non_zero()\n",
        "```\n",
        "\n",
        "This function will locate all non-zero FRP points and store them into a python object called a tuple.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKPYZNjo89Be"
      },
      "outputs": [],
      "source": [
        "def non_zero(rave_frp):\n",
        "    # locates all indicies for RAVE nc file that have non-zero FRP\n",
        "    # stores these in a tuple nzi\n",
        "    frp_0 = rave_frp[:,:]\n",
        "    ff = frp_0 > 0\n",
        "    ff.astype(np.int)\n",
        "    # Non-zero FRP index values from files\n",
        "    nzi = np.nonzero(ff)\n",
        "\n",
        "    return nzi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWq5nSGHHs-p"
      },
      "source": [
        "The next function is called\n",
        "\n",
        "```\n",
        "mergedata\n",
        "```\n",
        "\n",
        "as is the will confirm we have two input files ready to be used. It then calls a function called\n",
        "\n",
        "```\n",
        "create_proc\n",
        "```\n",
        "\n",
        "that returns a completed dataframe object. The dataframe object, called\n",
        "\n",
        "```\n",
        "df\n",
        "```\n",
        "\n",
        "will be the merged RAVE-RAP dataset for a specified hour. At the end of this function, we save and export the dataframe into a csv file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8offRd4Ul76P"
      },
      "outputs": [],
      "source": [
        "def mergedata(date, h, RAVE_file, outfile):\n",
        "    # open RAVE file\n",
        "    rave_data = nc.Dataset(RAVE_file)\n",
        "    print ('opened RAVE file')\n",
        "    df = create_proc(date, h, rap_file, rave_data)\n",
        "\n",
        "    df.to_csv(outfile, index=False)\n",
        "\n",
        "    # try and deal with memory issues\n",
        "    del df\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaUXvLVns2XG"
      },
      "source": [
        "Below is the last function that we will define, called\n",
        "\n",
        "```\n",
        "create_proc\n",
        "```\n",
        "\n",
        "and will do the bulk of the data processing. Here, we read through the RAVE file and for each FRP point, we go into the RAP file and pull the closest latitude and longitude point as well as the variables of interest. We only do a small subset here, but this is where code could be modified to pull any variables contained in the RAP files.\n",
        "\n",
        "The function states with creating an empty dataframe object to populate with all the curren hour's FRP values and RAP variables.\n",
        "\n",
        "The function then reads in all RAVE data, removes the mask that the data are provided with, and collects the non-zero FRP values. It stores these FRP values and the corresponding latitude and longeitude values into a little dataframe that is looped through by index.\n",
        "\n",
        "Next, the function enters the loop that cycles through the non-zero FRP values. First in this loop, a few lines of code are didctated to finding the nearest latitude and longitude RAP grid-cell points to the corresponding FRP latitude and longitude. Once those are found, their indicies are saved into a variable called\n",
        "\n",
        "```\n",
        "rap_close\n",
        "```\n",
        "\n",
        "that will be used to extract the closest RAP grid variable(s) to the RAVE FRP observation. This is done in an \"if\" loop because we are appending this information into the larger dataframe objeect. This is just one way in which to pair the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXVUytGpsJRr"
      },
      "outputs": [],
      "source": [
        "def create_proc(date, h, rap_file, rave_data):\n",
        "    df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    grbs = xr.open_dataset(rap_file,\n",
        "         engine='pynio',\n",
        "         lock=False,\n",
        "         backend_kwargs=dict(format=\"grib2\"),\n",
        "         )\n",
        "    print ('opened RAP file ', rap_file)\n",
        "\n",
        "    rave_frp = rave_data.variables['FRP_MEAN'][h,:,:] #masked size (1, 1133, 2500) MW\n",
        "    rave_latt = rave_data.variables['grid_latt'][:] #size (1133, 2500) #center grid that is 3km\n",
        "    rave_lont = rave_data.variables['grid_lont'][:] #size (1133, 2500) #center grid that is 3km\n",
        "\n",
        "    # remove the mask, which fills the null values with -1, and keeps values\n",
        "    rave_frp = rave_frp.filled()\n",
        "\n",
        "    # find the non-zero FRP indicies in RAVE\n",
        "    nzi = non_zero(rave_frp)\n",
        "\n",
        "    # storing all non-zero FRP points from other variables\n",
        "    frp_rave = rave_frp[(nzi[0]),(nzi[1])] # will be the length of the number of non-zero FRP points\n",
        "    frplatt_rave = rave_latt[(nzi[0]),(nzi[1])] # will be the length of the number of non-zero FRP points\n",
        "    frplont_rave = rave_lont[(nzi[0]),(nzi[1])] # will be the length of the number of non-zero FRP points\n",
        "\n",
        "    # set up dataframe of the non-zero RAVE variables for current hour and RAP variables\n",
        "    rave_df = pd.DataFrame(columns = ['grid_lont', 'grid_latt', 'FRP'])\n",
        "\n",
        "    # storing all those variables into the beginning of a dataframe\n",
        "    rave_df['FRP'] = frp_rave\n",
        "    rave_df['grid_latt'] = frplatt_rave\n",
        "    rave_df['grid_lont'] = frplont_rave\n",
        "\n",
        "    print('made rave dataframe rave_df')\n",
        "\n",
        "    first = int(0)\n",
        "\n",
        "    # Make a dataset from desired variable\n",
        "    ds = H.xarray(searchString=\"TMP:2 m\")\n",
        "\n",
        "    # loop through the RAVE FRP dataframe\n",
        "    for ind in rave_df.index:\n",
        "\n",
        "        lat_test = ds.latitude.values.flatten()\n",
        "        lon_test = ds.longitude.values.flatten()\n",
        "        rap_map = pd.DataFrame({\"raplat\": lat_test, \"raplon\": lon_test})\n",
        "\n",
        "        frp_lat = rave_df['grid_latt']\n",
        "        frp_lon = rave_df['grid_lont']\n",
        "        frp_pt = pd.DataFrame({\"Lat\": frp_lat, \"Lon\": frp_lon})\n",
        "        single = np.array((frp_pt.at[ind,'Lat'], frp_pt.at[ind,'Lon']))\n",
        "        single = single.reshape(1,2)\n",
        "\n",
        "        # single is [lat lont]\n",
        "        print('single: ', single)\n",
        "\n",
        "        # find the nearest RAP grid indicies to match RAVE data lat and lon\n",
        "        ln_test = lon_test-360\n",
        "        lt_test = lat_test-360\n",
        "        frp_lt = frp_pt.at[ind,'Lat']-360\n",
        "        frp_ln = frp_pt.at[ind,'Lon']-360\n",
        "        abslon = np.abs(ln_test - frp_ln)\n",
        "        abslat = np.abs(lt_test - frp_lt)\n",
        "        c = np.maximum(abslon, abslat)\n",
        "\n",
        "        idx = np.argmin(c)\n",
        "        print('idx: ', idx)\n",
        "        rap_close = idx\n",
        "        print('rap_close: ', rap_close)\n",
        "\n",
        "        if np.isnan(frp_lat[ind]):\n",
        "            rap_close = float(\"NAN\")\n",
        "\n",
        "        if first == 0:\n",
        "            first = int(1)\n",
        "            if np.isnan(rap_close):\n",
        "                data_df = pd.DataFrame({\"lat\": float(\"NAN\"),\n",
        "                                        \"lon\": float(\"NAN\"),\n",
        "                                        \"temp2m\": float(\"NAN\"),\n",
        "                                        \"DPT\": float(\"NAN\"),\n",
        "                                        \"SPFH\": float(\"NAN\"),\n",
        "                                        \"uwind\": float(\"NAN\"),\n",
        "                                        \"vwind\": float(\"NAN\"),\n",
        "                                        \"gust\": float(\"NAN\"),\n",
        "                                        \"SOILW\": float(\"NAN\"),\n",
        "                                        \"VGTYP\": float(\"NAN\"),\n",
        "                                        \"SOTYP\": float(\"NAN\"),\n",
        "                                        \"FRP\": float(\"NAN\"),\n",
        "                                        \"id\": float(\"NAN\"),\n",
        "                                        \"dateTime\": float(\"NAN\")}, index=[0])\n",
        "            else:\n",
        "\n",
        "                data_df = pd.DataFrame({\"lat\": grbs['gridlat_0'].values.flatten()[rap_close],\n",
        "                                        \"lon\": grbs['gridlon_0'].values.flatten()[rap_close],\n",
        "                                        \"temp2m\": ds.t2m.values.flatten()[rap_close],\n",
        "                                        \"DPT\": grbs['DPT_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"SPFH\": grbs['SPFH_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"uwind\": grbs['UGRD_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"vwind\": grbs['VGRD_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"gust\": grbs['GUST_P0_L1_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"SOILW\": grbs['SOILW_P0_2L106_GRLL0'][0,:,:].values.flatten()[rap_close],\n",
        "                                        \"VGTYP\": grbs['VGTYP_P0_L1_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"SOTYP\": grbs['SOTYP_P0_L1_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"FRP\": rave_df['FRP'][ind],\n",
        "                                        \"id\": rap_close,\n",
        "                                        \"dateTime\": [date]}, index=[0])\n",
        "            df = pd.DataFrame(data=data_df, index=[0])\n",
        "            del data_df\n",
        "        else:\n",
        "        # Append FRP and RAP together in a dataframe\n",
        "            if np.isnan(rap_close):\n",
        "                data_df2 = pd.DataFrame({\"lat\": float(\"NAN\"),\n",
        "                                        \"lon\": float(\"NAN\"),\n",
        "                                        \"temp2m\": float(\"NAN\"),\n",
        "                                        \"DPT\": float(\"NAN\"),\n",
        "                                        \"SPFH\": float(\"NAN\"),\n",
        "                                        \"uwind\": float(\"NAN\"),\n",
        "                                        \"vwind\": float(\"NAN\"),\n",
        "                                        \"gust\": float(\"NAN\"),\n",
        "                                        \"SOILW\": float(\"NAN\"),\n",
        "                                        \"VGTYP\": float(\"NAN\"),\n",
        "                                        \"SOTYP\": float(\"NAN\"),\n",
        "                                        \"FRP\": float(\"NAN\"),\n",
        "                                        \"id\": float(\"NAN\"),\n",
        "                                        \"dateTime\": float(\"NAN\")}, index=[0])\n",
        "            else:\n",
        "                data_df2 = pd.DataFrame({\"lat\": grbs['gridlat_0'].values.flatten()[rap_close],\n",
        "                                        \"lon\": grbs['gridlon_0'].values.flatten()[rap_close],\n",
        "                                        \"temp2m\": grbs['TMP_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"DPT\": grbs['DPT_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"SPFH\": grbs['SPFH_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"uwind\": grbs['UGRD_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"vwind\": grbs['VGRD_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"gust\": grbs['GUST_P0_L1_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"uwind\": grbs['UGRD_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"vwind\": grbs['VGRD_P0_L103_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"gust\": grbs['GUST_P0_L1_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"SOILW\": grbs['SOILW_P0_2L106_GRLL0'][0,:,:].values.flatten()[rap_close],\n",
        "                                        \"VGTYP\": grbs['VGTYP_P0_L1_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"SOTYP\": grbs['SOTYP_P0_L1_GRLL0'].values.flatten()[rap_close],\n",
        "                                        \"FRP\": rave_df['FRP'][ind],\n",
        "                                        \"id\": rap_close,\n",
        "                                        \"dateTime\": [date]}, index=[0])\n",
        "            hold_df_2 = pd.DataFrame(data=data_df2, index=[0])\n",
        "            df = df.append(hold_df_2, ignore_index=True)\n",
        "            del hold_df_2\n",
        "        # else it was a missing RAP and should be NAN\n",
        "    else:\n",
        "        data_df = pd.DataFrame({\"lat\": float(\"NAN\"),\n",
        "                                        \"lon\": float(\"NAN\"),\n",
        "                                        \"temp2m\": float(\"NAN\"),\n",
        "                                        \"DPT\": float(\"NAN\"),\n",
        "                                        \"SNOD\": float(\"NAN\"),\n",
        "                                        \"SPFH\": float(\"NAN\"),\n",
        "                                        \"uwind\": float(\"NAN\"),\n",
        "                                        \"vwind\": float(\"NAN\"),\n",
        "                                        \"gust\": float(\"NAN\"),\n",
        "                                        \"SOILW\": float(\"NAN\"),\n",
        "                                        \"VGTYP\": float(\"NAN\"),\n",
        "                                        \"SOTYP\": float(\"NAN\"),\n",
        "                                        \"FRP\": float(\"NAN\"),\n",
        "                                        \"id\": float(\"NAN\"),\n",
        "                                        \"dateTime\": float(\"NAN\")}, index=[0])\n",
        "\n",
        "    grbs.close()\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQUEB1gI9Sc7"
      },
      "source": [
        "1.2.2 Lastly, this is our main function. The main function allows us to loop through multiple files at multiple time steps to create RAVE-RAP hourly files. We will process two hours of files to create the ML and RF ready dataset.\n",
        "\n",
        "In the following code, we define our main function. In that main function, we loop through time. We check and grab our desired input data files. We then run a function, called\n",
        "\n",
        "```\n",
        "mergedata()\n",
        "```\n",
        "\n",
        "This function is where we call that large, longest function called\n",
        "\n",
        "```\n",
        "create_proc()\n",
        "```\n",
        "\n",
        "That is the function that we defined earlier in the notebook that does our heavy lifting. This function takes the date, RAP file, RAVE file, and output file name as inputs. With these inputs, it will check and read both the selected RAVE and RAP files for the same hour. It will then match the RAVE data location with RAP gridcell, select the RAP variables of interest for the location, create a dataframe object listed by the non-zero FRP points, and finally export/save the dataframe into a csv format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpTaDQpP9bBb"
      },
      "outputs": [],
      "source": [
        "# define a main function %\n",
        "# We are only processing two hours of files from 9-1-22\n",
        "year = 2021\n",
        "def main():\n",
        "    for m in range(9, 10):       # specify your month range\n",
        "        for d in range(1, 2):   # specify your day range\n",
        "            for h in range(1,2): # the files are daily for RAVE but we want hourly files produced\n",
        "                try:\n",
        "                    date = datetime(year, m, d, h)\n",
        "                    # recall that we have already saved the RAVE file in a 1.1.1 as \"RAVE_nc\"\n",
        "                    RAVE_file = RAVE_nc\n",
        "                    print ('pulled RAVE file', RAVE_file)\n",
        "                    print('Working on date: ', date)\n",
        "                    # Use Herbie to create our RAP input file\n",
        "                    #H = Herbie(\"2024-01-01 00:00\", model=\"rap\", fxx=0)\n",
        "                    rap_file = Herbie(\"%s %s:00\", model=\"rap\" , fxx=0 %(date.strftime(\"%Y-%m-%d\"), date.strftime(\"%H\")))\n",
        "                    #rap_file = os.path.join(rap_directory, \"%s%s\" % (date.strftime(\"%y%j\"), \"23000000\"))\n",
        "                    print(\"I found the RAP data: \", rap_file)\n",
        "\n",
        "                    outfile = os.path.join(out_directory, \"%s/%s.csv\" % (date.strftime(\"%Y%m%d\"), date.strftime(\"%y%m%d%H\")))\n",
        "                    mergedata(date, h, RAVE_file, outfile)\n",
        "                    print (\"Wrote to %s\" % outfile)\n",
        "\n",
        "\n",
        "                except ValueError as e:\n",
        "                    pass\n",
        "                    print(\"date for file doesn't exist\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X3codJF7XdG"
      },
      "source": [
        "Congratulations! You have now exported hourly csv files, containing the hourly dataframe tables of FRP and RAP data, which can be used to train a machine learning model such as the random forest.\n",
        "\n",
        "From these files, a number of computations can be made and/or applications could done. For instance, one computation can be if we are interested in calculating a rolling 24-hour average FRP value, then we can write code to read each hour for 24 hours, sum the non-zero FRP for a same latitude and longitude grid value, and then finally the mean over that grid. In fact, that is a value found in section 2 as an input into a demo RF model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFhmy0FLG3Ga"
      },
      "source": [
        "1.2.3 If you desire to merge these csv files into a larger, singular dataset csv file. The code can be found below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = './'\n",
        "extension = '.csv'\n",
        "\n",
        "files = [file for file in os.listdir(path) if file.endswith(extension)]\n",
        "\n",
        "dfs = []\n",
        "for file in files:\n",
        "    df = pd.read_csv(os.path.join(path, file))\n",
        "    dfs.append(df)\n",
        "frp_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "frp_all.to_csv('./RAVE_RAP_dataset.csv', index = False)"
      ],
      "metadata": {
        "id": "UQBiILaQle62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoVppmquB-PX"
      },
      "source": [
        "# 2.1 Training a Random Forest for FRP from the RAVE-RAP Dataset Example\n",
        "\n",
        "**In this section, we import a large RAVE-RAP dataset created from data in July 2018 through December 2021. We will then train RF models from this dataset and visualize their performance on blind validation datasets and datasets that meet certain specified credientials.**\n",
        "\n",
        "**MUST DO BEFORE PROCEEDING**: You are going to need to change the colab compute resources to complete the model training in this colab workspace in a timely manner. To do this:\n",
        "\n",
        "> 1) Look to the upper right, possibly below comment and share if you have not hidden the colab header, where it shows the RAM and Disk icons. **Click the RAM and Disk icons**, not the dropdown arrow, to reveal the Resources tab.\n",
        "\n",
        "> 2) At the bottom of that tab is an option that says Change runtime type. Click **Change runtime type** and select **T4GPU** and then select okay, and then finally press save.\n",
        "\n",
        "You can then close the Resources tab by clicking the X next to Resources.\n",
        "\n",
        "To do this, we first need to gather the AI ready dataset from Zenodo: https://zenodo.org/records/10463552 . Note that this step might take several minutes in total. The first wget request will get the dataset, which we will call:\n",
        "\n",
        "```\n",
        "frp_all\n",
        "```\n",
        "\n",
        "Before we make the wget request, we will start from scratch and clear out everything we did in our working space from sections 0 and 1. This inlcudes removing from the working space all previous variables and ensuring that all python tools have been re-imported. You will want to type \"y\" once prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0ti5Sbn12EW"
      },
      "outputs": [],
      "source": [
        "%reset\n",
        "\n",
        "!pip install wget\n",
        "\n",
        "import sys\n",
        "import wget\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjIcZo9R-TZP"
      },
      "source": [
        "2.1.1 Next, we will get and download the RF-ready dataset and save it as a dataframe object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KeSXybe-auo"
      },
      "outputs": [],
      "source": [
        "frp_csv = wget.download('https://zenodo.org/records/10790965/files/RAVE_RAP_2019_2021.csv?download=1')\n",
        "frp_all = pd.read_csv(frp_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can click the folder icon on the far left to see that the csv file has been got from the Zenodo database and saved to the colab space. You can also note how large the file is.\n",
        "\n",
        "Lastly, we will check for any NaN values and drop those rows from the dataset."
      ],
      "metadata": {
        "id": "CdVJARzWulu7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g4Ai108k1EE"
      },
      "outputs": [],
      "source": [
        "## drop the rows with NaNs\n",
        "frp_all.dropna(axis=0, subset=['rave_frp', 'time','lat', 'lon', 'hour', 'temp2m', 'wind_gust', 'precip', 'dpt', 'uwind', 'vwind',\n",
        "                                'temp_surf', 'snod', 'spfh', 'pwat', 'weasd', 'dswrf', 'uswrf', 'dlwrf', 'ulwrf',\n",
        "                                'cape', 'ltng', 'tsoil', 'soilw', 'vgtyp', 'sotyp',\n",
        "                                'yester_frp', '24hr_roll_frp'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU_akQUgktZI"
      },
      "source": [
        "2.1.2. In this section, here is where we can modify the dataset for whatever training conditions we wish to apply. This is where understanding your input data becomes important. In our example, the RAVE dataset includes all observed hourly FRP detections, and while it is a well created dataset, it can include false oberservations and detections as well as encounter other errors and biases associated with satellite retreived FRP values. One such example might be satellite signatures such as bright flares, etc. can register as weaker FRP values.\n",
        "\n",
        "In this training this ML model, we are attempting to model hourly FRP with known previous FRP information as well as current weather information from RAP. As is similarly seen in other operations, a good way to ensure a comprehensive understanding of previous FRP on a gridded domain is to take a mean of previously observed FRP over some period of time. This helps avoid missed hourly intervals due to cloud or smoke blocking, satellite orbit, etc. There are two mean values included in this dataset:\n",
        "> **yester_frp** which is a mean FRP value that was calulated from 0-23 UTC of the day-before hourly observed FRPs from the corresponding gridcell of the current hour FRP.\n",
        "\n",
        "> **24hr_roll_frp** which is a mean FRP value that was was calculated as a mean up to the hour before the listed FRP value and observation.\n",
        "\n",
        "These pre-calulated values can help refine our ML model training process to make sure that is could perform in real time, since it isn't dependent on a singular previous hour FRP, and help compensate for some of the satellite and/or missing value issues.\n",
        "\n",
        "[reference my publicaiton here?]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvA8FagNlB11"
      },
      "outputs": [],
      "source": [
        "## Create a dataset for training data with conditionals\n",
        "\n",
        "## REQUIRED: drop rows where 24hr_roll_frp = 0\n",
        "frp_all = frp_all.loc[~((frp_all['24hr_roll_frp'] <= 0))]\n",
        "frp_all['24hr_roll_frp'] = frp_all['24hr_roll_frp'].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsU-P3W5kuB-"
      },
      "source": [
        "The following code block is optional code and all commented out. We can specify any additional conditionals to apply to our dataset, if we have interst in training a model that specifically looks at certain regions or has certain conditions. This can include location, input variable thresholds, etc. In this case, there is an example of how to specify a certain region. To add conditionals, either uncomment the single \"#\" exisiting code below, or inset your own. Then execute the block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E35Cxf_WlNt-"
      },
      "outputs": [],
      "source": [
        "## -- Here are examples where we can specify the region -- ##\n",
        "## drop the lons east of 105\n",
        "\n",
        "#indexNames = frp_all[frp_all['lon'] >= -105 ].index\n",
        "#frp_all.drop(indexNames , inplace=True)\n",
        "#size_frp_all_3 = frp_all.shape\n",
        "#print('size of western lons: ', size_frp_all_3)\n",
        "## drop the lats south of 25\n",
        "#indexNames = frp_all[frp_all['lat'] <= 25 ].index\n",
        "#frp_all.drop(indexNames , inplace=True)\n",
        "#size_frp_all_3 = frp_all.shape\n",
        "#print('size of northern lats: ', size_frp_all_3)\n",
        "\n",
        "## -- Here are examples where we can specify other conditionals, like\n",
        "## -- selecting points only where a fire was burning the time before -- ##\n",
        "## drop rows where 24hr_roll_frp = 0\n",
        "#frp_all = frp_all.loc[~((frp_all['24hr_roll_frp'] <= 0))]\n",
        "#frp_all['24hr_roll_frp'] = frp_all['24hr_roll_frp'].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xMfWK4lHbVp"
      },
      "source": [
        "Lastly, we will extract some specific dates to remove from our machine learning training and testing process. These dates can later be referenced for the final blind-validation of the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "467l-rQvH7wx"
      },
      "outputs": [],
      "source": [
        "# Specify the desired time(s) to withold for validation\n",
        "t1 = frp_all[(frp_all['time'] <= '2019-11-01') & (frp_all['time'] > '2019-08-01')]\n",
        "t2 = frp_all[(frp_all['time'] <= '2020-09-25') & (frp_all['time'] > '2020-08-25')]\n",
        "frp_validate = pd.concat([t1, t2])\n",
        "\n",
        "# Remove those same times from the main dataset\n",
        "frp_all=frp_all[~(frp_all['time'] <= '2019-11-01') & (frp_all['time'] > '2019-08-01')]\n",
        "frp_all=frp_all[~(frp_all['time'] <= '2020-09-25') & (frp_all['time'] > '2020-08-25')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NBtLAP3_peG"
      },
      "outputs": [],
      "source": [
        "## check the number of total samples for testing and training\n",
        "n_samples = frp_all.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVlmd28BkuR0"
      },
      "source": [
        "2.1.3 Here is where we will train the RF model to predict the next-hour FRP. This time, we are using the pre-loaded machine-learning ready dataset that we just created. To reference Regrssor options in depth, please visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "\n",
        "This code takes around 15-20 minutes to run as written. If you changed the estimators and random state, it might take your model less or more time to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JLBj9yNm0PV"
      },
      "outputs": [],
      "source": [
        "## Create the list of features we will have as inputs\n",
        "feature_list = list(['lat', 'lon', 'hour', 'temp2m', 'wind_gust', 'precip', 'dpt', 'uwind', 'vwind',\n",
        "                                'temp_surf', 'snod', 'spfh', 'pwat', 'weasd', 'dswrf', 'uswrf', 'dlwrf', 'ulwrf',\n",
        "                                'cape', 'ltng', 'tsoil', 'soilw', 'vgtyp', 'sotyp', '24hr_roll_frp'])\n",
        "\n",
        "## Create the features dataframe we will use as inputs\n",
        "features = frp_all[['lat', 'lon', 'hour', 'temp2m', 'wind_gust', 'precip', 'dpt', 'uwind', 'vwind',\n",
        "                                'temp_surf', 'snod', 'spfh', 'pwat', 'weasd', 'dswrf', 'uswrf', 'dlwrf', 'ulwrf',\n",
        "                                'cape', 'ltng', 'tsoil', 'soilw', 'vgtyp', 'sotyp', '24hr_roll_frp']]\n",
        "\n",
        "## Labels\n",
        "labels=frp_all['rave_frp']\n",
        "\n",
        "\n",
        "## Split dataset into training set and test set\n",
        "indices = np.arange(n_samples)\n",
        "train_features, test_features, train_labels, test_labels, idx1, idx2 = train_test_split(features, labels, indices, test_size=0.3) # 70% training and 30% test\n",
        "\n",
        "print('Training Features Shape:', train_features.shape)\n",
        "print('Training Labels Shape:', train_labels.shape)\n",
        "print('Testing Features Shape:', test_features.shape)\n",
        "print('Testing Labels Shape:', test_labels.shape)\n",
        "\n",
        "## RF model where we can specify any number of parameters.\n",
        "## n_estimators = number of trees in the forest\n",
        "## random_state = integer chosen to insure reproducable results\n",
        "rf_model = RandomForestRegressor(n_estimators = 100, random_state = 10)\n",
        "\n",
        "## Train the model on training data\n",
        "rf_model.fit(train_features, train_labels);\n",
        "\n",
        "## Use the forest's predict method on the test data\n",
        "predictions = rf_model.predict(test_features)\n",
        "all_frp = rf_model.predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmvXIQmFlWIa"
      },
      "source": [
        "\n",
        "Congratulations, you have trained your first random forest model to take a set of inputs from current hour weather conditions and previous hours' FRP information, and model the current hour FRP.\n",
        "\n",
        "To see how the model did on the testing dataset, run the following code to check out the errors of the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft1ZgAdGm-Gf"
      },
      "outputs": [],
      "source": [
        "## Calculate the absolute errors\n",
        "errors = abs(predictions - test_labels)\n",
        "## Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error for FRP:', round(np.mean(errors), 2))\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(test_labels, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8caHgq7anHHM"
      },
      "source": [
        "\n",
        "If you want to save the model, this is how to do it. Trained ML models are large files. In the next sub-section, we will practice opening the saved model and using it to make hourly FRP predictions on the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZkIhPDEnNyN"
      },
      "outputs": [],
      "source": [
        "## Save the model to disk\n",
        "filename = './RF_RAVE_RAP_MODEL_TUTORIAL.sav'\n",
        "\n",
        "print('saving model: ', filename)\n",
        "n_bytes = 2**31\n",
        "max_bytes = 2**31 - 1\n",
        "data = bytearray(n_bytes)\n",
        "## write to the file\n",
        "bytes_out = pickle.dumps(rf_model)\n",
        "with open(filename, 'wb') as f_out:\n",
        "    for idx in range(0, len(bytes_out), max_bytes):\n",
        "        f_out.write(bytes_out[idx:idx+max_bytes])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1.4 We can also check the errors of the trained model against the dataset that we held back. Since this data was completely withheld, this process is called *inference* and is an example how a pretrained model can be used on future or unseen data in application. As is the case with any pretrained models, our input variable selection must be the same input variable selection as the trained model. We cannot only add a few inputs or add extra inputs. It will cause an error.\n",
        "\n"
      ],
      "metadata": {
        "id": "2UdMu3B_nVId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## define the input variables\n",
        "validate_features = frp_validate[['lat', 'lon', 'hour', 'temp2m', 'wind_gust', 'precip', 'dpt', 'uwind', 'vwind',\n",
        "                                'temp_surf', 'snod', 'spfh', 'pwat', 'weasd', 'dswrf', 'uswrf', 'dlwrf', 'ulwrf',\n",
        "                                'cape', 'ltng', 'tsoil', 'soilw', 'vgtyp', 'sotyp', '24hr_roll_frp']]\n",
        "\n",
        "## define the target or output variable\n",
        "validate_labels = frp_validate['rave_frp']\n",
        "\n",
        "## run the inference stage and save output into a new column in the validate dataframe\n",
        "validate_result = rf_model.predict(validate_features)\n",
        "frp_validate['RF_FRP'] = validate_result\n",
        "print('finished creating the RF FRP values')\n",
        "\n",
        "## Calculate the absolute errors\n",
        "errors = abs(validate_result - validate_labels)\n",
        "## Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error for FRP for validation dataset:', round(np.mean(errors), 2))\n",
        "print('Mean Squared Error for validation dataset:', metrics.mean_squared_error(validate_labels, validate_result))"
      ],
      "metadata": {
        "id": "QIbkbioDnUv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is example code that shows how to open a previously saved RF model, saved with the name/locaiton of *filename*, and run the inference stage on a set of input variables called *test_features*.\n",
        "\n",
        "```\n",
        "## open the previously saved RF model\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.predict(test_features)\n",
        "```\n"
      ],
      "metadata": {
        "id": "-LVm_r8t0734"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have successfully completed the interference stage. Remember, you can return to the begining of section 2.1.2 to apply more conditionals to the dataset. Try rerunning section 2.1 with different inputs and conditionals!"
      ],
      "metadata": {
        "id": "dVYqIJ67odoa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd21SHsoIb3p"
      },
      "source": [
        "# 2.2 Analyze the performance of the RF Model\n",
        "\n",
        "In this section, we will apply different tools to the trained RF model to analyze its performance. You will have needed to train the RF model in section 2.1 to apply these tools to the RF model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.1 In this subsection, we will step through the short code to calculate the importances of the input variables, or features, in the RF model. The importances will sum to 1 and are ranked as most important in determining the hourly FRP value to least important. It is not a direct scientific correlation to hourly FRP and the variables we have used to train the mode. It is a direct measure of what the model has determined as the most important to least important features in calculating the hourly FRP. To learn more about this tool in python, please reference:"
      ],
      "metadata": {
        "id": "M_W1rNiG0a57"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WbB1_vToKcv"
      },
      "outputs": [],
      "source": [
        "## Get numerical feature importances\n",
        "importances = list(rf_model.feature_importances_)\n",
        "\n",
        "## List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
        "\n",
        "## Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "## Print out the feature and importances\n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.2 Here, we can visualize the performance of the model on the blind validation dataset we reserved. This plotting code will plot the truth, or the FRP retrieval for that gridcell at that hour, compared to our RF modeled FRP value for that hour."
      ],
      "metadata": {
        "id": "1J1u1Zue7uBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the 1:1 line\n",
        "lineStart = validate_labels.min()\n",
        "lineEnd = validate_labels.max()\n",
        "\n",
        "## Check out the actual test versus predictors with 1:1 line\n",
        "fig1 = plt.gcf()\n",
        "plt.scatter(validate_labels, validate_result, c='b', marker='x', alpha=0.15, label='RF')\n",
        "line = mlines.Line2D([0, 1], [0, 1], color='red')\n",
        "plt.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-', color = 'r')\n",
        "plt.xlim(lineStart, lineEnd)\n",
        "plt.ylim(lineStart, lineEnd)\n",
        "plt.xlabel('Truth FRP (MW)'); plt.ylabel('Predicted FRP (MW)')\n",
        "plt.title('RAVE Observed Versus Modeled FRP')\n",
        "plt.legend(loc=\"upper left\", title='Models')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "MQX6X2zEm4o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHRISTINA FINISH THIS SECITON** The plot shows us the spread of predicted values at higher FRP values, which are associated with more powerfully burning wildfires. The goal of the model would be to have the prediction as close to the truth, following the 1:1 line as possible. What trends do you notice about the model you trained?\n",
        "\n",
        "You can determine a statistical threshold for FRP to perform analysis on higher FPR values. If we set one arbitraily at 500 MW. This allows for slightly easier analysis of performance at the higher, more interesting FRP points."
      ],
      "metadata": {
        "id": "DEhPiWaWNe4G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OIYsLmYB4IH"
      },
      "source": [
        "# References\n",
        "\n",
        "Blaylock, B. K. (2022). Herbie: Retrieve Numerical Weather Prediction Model Data (Version 2022.9.0) [Computer software]. https://doi.org/10.5281/zenodo.4567540\n",
        "\n",
        "Fangjun Li, Xiaoyang Zhang, Shobha Kondragunta, Xiaoman Lu, Ivan Csiszar, Christopher C. Schmidt, Hourly biomass burning emissions product from blended geostationary and polar-orbiting satellites for air quality forecasting applications, Remote Sensing of Environment, Volume 281, 2022,113237, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2022.113237.\n",
        "(https://www.sciencedirect.com/science/article/pii/S0034425722003431)\n",
        "\n",
        "Kumler-Bonfanti FRP PAPER HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}